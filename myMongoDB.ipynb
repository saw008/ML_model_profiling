{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "myMongoDB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pXxebZbHhHm2",
        "AaDuVplQnWmc",
        "Yo1zBa8MnUw7",
        "F2MgD3K2LfDb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWcY3aEChEr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d16bc8-af3e-4a79-c0b4-43ef94e2abb9"
      },
      "source": [
        "# To obtain the IP address here\n",
        "!curl ipecho.net/plain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34.90.114.41"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD7JZUnfkZLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d173f1-5c53-4816-be0b-31aff6b014a7"
      },
      "source": [
        "!pip3 install pymongo[srv]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.6/dist-packages (3.11.2)\n",
            "Collecting dnspython<2.0.0,>=1.16.0; extra == \"srv\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d3/3aa0e7213ef72b8585747aa0e271a9523e713813b9a20177ebe1e939deb0/dnspython-1.16.0-py2.py3-none-any.whl (188kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20kB 20.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 18.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 40kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 71kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 102kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 112kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 122kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 133kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 143kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 153kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 163kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 174kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 184kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 6.1MB/s \n",
            "\u001b[?25hInstalling collected packages: dnspython\n",
            "Successfully installed dnspython-1.16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3pO25ukjjzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bdb145a-3be7-429c-d333-276757ced1a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pymongo\n",
        "\n",
        "# insert_one(), insert_many(), update_one(), update_many(), replace_one(), delete_one(), delete_many()\n",
        "# The following links are basic 101 to mongoDB x Python, in Chinese.\n",
        "# https://blog.csdn.net/culiu9261/article/details/107540063\n",
        "# https://www.runoob.com/python3/python-mongodb.html\n",
        "\n",
        "my_access_pwd = \"FNhkwCvGlAnRcntG\"\n",
        "my_url = \"mongodb+srv://saw008:\"+ my_access_pwd + \"@cluster0.9vbsx.mongodb.net/sampleTable?retryWrites=true&w=majority\"\n",
        "client = pymongo.MongoClient(my_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts51cRF9Fx78"
      },
      "source": [
        "下面开始认真了！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8XjNqI3eq58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d657067a-97ea-4cff-a8b8-cacb44cae8fb"
      },
      "source": [
        "# Get all the databases in my cluster.\n",
        "print(client.list_database_names())\n",
        "table_data = client['thesis']['data_table']\n",
        "table_model = client['thesis']['model_table']\n",
        "table_perf = client['thesis']['performance_table']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thesis', 'admin', 'local']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSLqjoitfc_n"
      },
      "source": [
        "# To copy files or folders in google drive, you can use the following command.\n",
        "# !cp -r \"/src/folder\" \"/dest/folder\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXxebZbHhHm2"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQWvPyWyi3Kr"
      },
      "source": [
        "# The following functions is for inserting data_table\n",
        "import scipy.io as sciio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import numpy as np\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "\n",
        "def load_freq_data(scenario_path):\n",
        "  folder = scenario_path + '/' if not scenario_path.endswith('/') else scenario_path\n",
        "  data_file = folder + \"PMUData.mat\"\n",
        "  csv_file = folder + \"ScenarioDescription.csv\"\n",
        "  mat_file_contents = sciio.loadmat(data_file)\n",
        "  pmu_locations = mat_file_contents['fmeas_con'][:, 1].astype(int)\n",
        "  num_time_steps, num_bus = mat_file_contents['PMU']['f'][0][0].shape\n",
        "  scenario_description = pd.read_csv(csv_file)\n",
        "  load_changes_start_times = scenario_description['Start time(s) for load changes'][0]\n",
        "  frequency_data = mat_file_contents['PMU']['f'][0][0][load_changes_start_times * 50 + 25: num_time_steps, pmu_locations - 1]\n",
        "  return frequency_data\n",
        "\n",
        "def pattern_obtainer(data):\n",
        "  precision = 4\n",
        "  min_val = np.around(np.min(data), decimals=precision)\n",
        "  max_val = np.around(np.max(data), decimals=precision)\n",
        "  avg_val = np.around(np.mean(data), decimals=precision)\n",
        "  std_val = np.around(np.std(data), decimals=precision)\n",
        "  sample_count = data.shape[0]\n",
        "  return min_val, max_val, avg_val, std_val, sample_count\n",
        "  \n",
        "def entry_dict_data(ifTest, pattern, measurement, dataPath):\n",
        "  return {\"pattern\": {\n",
        "               \"min\": pattern[0],\n",
        "               \"max\": pattern[1],\n",
        "               \"avg\": pattern[2],\n",
        "               \"std\": pattern[3],\n",
        "               \"no_of_samples\": pattern[4]\n",
        "               }, \n",
        "          \"measurement\": measurement, \n",
        "          \"data_path\": dataPath}\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def entry_dict_model(training_data, model_type, model_loc, task_category, training_time, learning_rate, batch_size, epoch_number, applied_libs):\n",
        "  return {\"training_data\": training_data,\n",
        "          \"model_type\": model_type,\n",
        "          \"model_loc\": model_loc,\n",
        "          \"task_category\": task_category,\n",
        "          \"training_time\": training_time,\n",
        "          \"learning_rate\": learning_rate,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"epoch_number\": epoch_number,\n",
        "          \"applied_libs\": applied_libs\n",
        "          }\n",
        "def data_distinguisher():\n",
        "  # read the csv file\n",
        "  with open(\"/content/drive/My Drive/myThesis/test001/Testing/FullScenarioGeneration.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    # extract the skipped scenarios\n",
        "    temp_list = []\n",
        "    for item in reader:\n",
        "        # skip the first line(title)\n",
        "        if reader.line_num == 1:\n",
        "            continue\n",
        "        if item[6] != '--' and item[6] != '':\n",
        "          tmp = item[6]\n",
        "          if '\\n' in tmp:\n",
        "            tmp = tmp.replace('\\n', ' ')\n",
        "          if ',' in tmp:\n",
        "            temp_list = temp_list + [int(x) for x in tmp.split(', ')]\n",
        "          else:\n",
        "            temp_list.append(int(item[6]))\n",
        "  while temp_list[-1] > 450:\n",
        "    temp_list.pop(-1)\n",
        "  for i in range(1, 451):\n",
        "    if (i%15 == 14) or (i%15 == 0):\n",
        "      temp_list.append(i)\n",
        "  temp_list=list(set(temp_list))\n",
        "  # print(temp_list)\n",
        "  ## 现在不要的scenario是temp_list\n",
        "  return temp_list\n",
        "  \n",
        "def train_data_obtainer(length, inx):\n",
        "  non_train_data = set(data_distinguisher())\n",
        "  # 这个samples只是n~n+45\n",
        "  samples = set(((inx-1)*length+np.array(range(1, length+1))).tolist())\n",
        "  samples = samples - samples.intersection(non_train_data)\n",
        "  \n",
        "  return list(samples)\n",
        "\n",
        "def read_xlsx(file_now):\n",
        "    wb = load_workbook(file_now)\n",
        "    sheets = wb.worksheets   # 获取当前所有的sheet\n",
        "    sheet1 = sheets[0]\n",
        "    # val = read_xlsx_cell(sheet1, row, column)\n",
        "    return sheet1\n",
        "\n",
        "\n",
        "def read_xlsx_cell(sheet1, row, column):\n",
        "  val = str(sheet1.cell(row, column).value)\n",
        "  return val\n",
        "#   return float((val)[:-1]) if val.endswith('s') else float(val)\n",
        "\n",
        "def entry_dict_perf(model_id, test_data, task_dict):\n",
        "  return {\n",
        "          \"model_id\": model_id, # TODO\n",
        "          \"test_data\": test_data, # str to the path (or ID) of the testing data\n",
        "          \"task\": task_dict # Dict\n",
        "          }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GADmHzrqM7MG"
      },
      "source": [
        "table_data = client['thesis']['data_table']\n",
        "table_model = client['thesis']['model_table']\n",
        "table_perf = client['thesis']['performance_table']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaDuVplQnWmc"
      },
      "source": [
        "####Data Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jn2cvDdS5_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87885d9-1e02-4f2d-d446-5488a3924380"
      },
      "source": [
        "import time\n",
        "table_data = client['thesis']['data_table']\n",
        "temp = []\n",
        "m = 0\n",
        "\n",
        "start = time.time()\n",
        "non_training_data = data_distinguisher()\n",
        "for i in range(1, 451):\n",
        "  folder_path = \"/content/drive/My Drive/myThesis/test001/Training/scenario\" + str(i)\n",
        "  try:\n",
        "    this_data = load_freq_data(folder_path)\n",
        "    # if_test = False if i in non_training_data else True\n",
        "    temp.append(entry_dict_data(None, pattern_obtainer(this_data), 'frequency', folder_path))\n",
        "  except:\n",
        "    # print('sb')\n",
        "    continue\n",
        "  \n",
        "# print(temp)\n",
        "table_data.insert_many(temp)\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.375295400619507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1kY6UerT03u"
      },
      "source": [
        "table_data = client['thesis']['data_table']\n",
        "table_data.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo1zBa8MnUw7"
      },
      "source": [
        "####Model Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGJUqX96uqcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138bd945-a2e3-4ed6-ef47-e4c3e9833f77"
      },
      "source": [
        "# from openpyxl import load_workbook\n",
        " \n",
        "# wb = load_workbook('/content/drive/My Drive/myThesis/test001/Testing/power_10Models.xlsx')\n",
        "# sheets = wb.worksheets   # 获取当前所有的sheet\n",
        "# print(sheets)\n",
        "\n",
        "# # 获取第一张sheet\n",
        "# sheet1 = sheets[0]\n",
        "# # sheet1 = wb['Sheet']  # 也可以通过已知表名获取sheet\n",
        "# print(sheet1)\n",
        "\n",
        "# # 通过Cell对象读取\n",
        "# cell_11 = sheet1.cell(2,4).value\n",
        "# print(cell_11)\n",
        "# cell_11 = sheet1.cell(1,2).value\n",
        "# print(cell_11)\n",
        "\n",
        "\n",
        "# # https://blog.csdn.net/liuyingying0418/article/details/101066630"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<Worksheet \"Sheet1\">]\n",
            "<Worksheet \"Sheet1\">\n",
            "1174.33s\n",
            "500-MAE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH1XvDnhUDV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7313ab-6abd-43c5-93bd-b31ac14a0c51"
      },
      "source": [
        "# Adding the 10 models\n",
        "import time\n",
        "import glob\n",
        "\n",
        "table_model = client['thesis']['model_table']\n",
        "temp = []\n",
        "m = 0\n",
        "\n",
        "start = time.time()\n",
        "# model_loc = glob.glob('/content/drive/MyDrive/myThesis/test001/Training/10_trained_models/saved10Model/*.meta')\n",
        "# for i in range(len(model_loc)):\n",
        "#   model_loc[i] = model_loc[i].replace('/content/drive/MyDrive/myThesis/test001/Training/10_trained_models/saved10Model', '~')\n",
        "for i in range(1, 11):\n",
        "  val = read_xlsx('/content/drive/My Drive/myThesis/test001/Testing/power_10Models.xlsx', 6*(i-1)+2, 4)\n",
        "  tmp_val = float((val)[:-1]) if val.endswith('s') else float(val)\n",
        "  tmp_train_data_index_list = train_data_obtainer(45, i)\n",
        "  tmp_train_data_id_list = []\n",
        "  model_loc = glob.glob('/content/drive/MyDrive/myThesis/test001/Training/10_trained_models/model'+str(i)+'-*')[0]\n",
        "  for item in tmp_train_data_index_list:\n",
        "    tmp_path = '/content/drive/My Drive/myThesis/test001/Training/scenario' + str(item)\n",
        "    tmp_train_data_id_list.append(table_data.find_one({'data_path': tmp_path})['_id'])\n",
        "  temp.append(entry_dict_model(tmp_train_data_id_list, \"STGCN\", model_loc, \"prediction\", tmp_val, 0.001, 1000, 30, [\"TensorFlow\"]))\n",
        "# print(temp)\n",
        "table_model.insert_many(temp)\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68.08313608169556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21d56OiYAwOl",
        "outputId": "b13f7684-8172-43e6-81fa-b2da6c52a51a"
      },
      "source": [
        "# Adding the 30 models\n",
        "import time\n",
        "import glob\n",
        "\n",
        "table_model = client['thesis']['model_table']\n",
        "temp = []\n",
        "m = 0\n",
        "\n",
        "start = time.time()\n",
        "for i in range(1, 31):\n",
        "  val = read_xlsx('/content/drive/MyDrive/myThesis/test001/Testing/power_30Models.xlsx', 2*i, 9)\n",
        "  tmp_val = float((val)[:-1]) if val.endswith('s') else float(val)\n",
        "  tmp_model_id = read_xlsx('/content/drive/MyDrive/myThesis/test001/Testing/power_30Models.xlsx', 2*i, 11)\n",
        "  tmp_train_data_index_list = train_data_obtainer(15, i)\n",
        "  tmp_train_data_id_list = []\n",
        "  model_loc = '/content/drive/MyDrive/myThesis/test001/Training/saved_30models/' + tmp_model_id\n",
        "  for item in tmp_train_data_index_list:\n",
        "    tmp_path = '/content/drive/My Drive/myThesis/test001/Training/scenario' + str(item)\n",
        "    tmp_train_data_id_list.append(table_data.find_one({'data_path': tmp_path})['_id'])\n",
        "  temp.append(entry_dict_model(tmp_train_data_id_list, \"STGCN\", model_loc, \"prediction\", tmp_val, 0.001, 1000, 30, [\"TensorFlow\"]))\n",
        "# print(temp)\n",
        "table_model.insert_many(temp)\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79.97550439834595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L07C5ISqW_rj"
      },
      "source": [
        "table_model = client['thesis']['model_table']\n",
        "table_model.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2MgD3K2LfDb"
      },
      "source": [
        "####Performance Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWr8_x4yfaDN",
        "outputId": "874be2e7-ef36-4023-8807-152d1330f681"
      },
      "source": [
        "# perf 10 models\n",
        "import time\n",
        "import openpyxl\n",
        "\n",
        "wb = load_workbook('/content/drive/My Drive/myThesis/test001/Testing/power_10Models.xlsx')\n",
        "sheet1 = wb.worksheets[0]\n",
        "rows, columns = sheet1.max_row, sheet1.max_column\n",
        "\n",
        "table_perf = client['thesis']['performance_table']\n",
        "temp = []\n",
        "\n",
        "start = time.time()\n",
        "for i in range(1, rows):\n",
        "  tmp_model_id = read_xlsx('/content/drive/MyDrive/myThesis/test001/Testing/power_30Models.xlsx', i+1, 11)\n",
        "  model_id = table_model.find_one({'model_loc': {'$regex': tmp_model_id}})['_id']\n",
        "  # \n",
        "\n",
        "  # obtaining individual ObjectID of test_data\n",
        "  tmp_test_data_index = int(float(read_xlsx(i+1, 1)))\n",
        "  tmp_path = '/content/drive/My Drive/myThesis/test001/Training/scenario' + str(tmp_test_data_index)\n",
        "  test_data = table_data.find_one({'data_path': tmp_path})['_id']\n",
        "\n",
        "  temp_metric_dict = {'task_category': 'prediction',\n",
        "                      '500-MAE': float(read_xlsx(i+1, 2)),\n",
        "                      '500-RMSE': float(read_xlsx(i+1, 3)),\n",
        "                      'Inference_Time': float((read_xlsx(i+1, 5))[:-1]),\n",
        "                      'epoch_to_quality': int(float(read_xlsx(i+1, 7)))\n",
        "                      }\n",
        "  temp.append(entry_dict_perf(model_id, test_data, temp_metric_dict))\n",
        "\n",
        "table_perf.insert_many(temp)\n",
        "end = time.time()\n",
        "# print(temp)\n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17.389697313308716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyLxlR8mGFim",
        "outputId": "22554c1e-1ea9-4231-9280-db611c6df546"
      },
      "source": [
        "# perf 30 models\n",
        "import time\n",
        "import openpyxl\n",
        "\n",
        "wb = load_workbook('/content/drive/My Drive/myThesis/test001/Testing/power_30Models.xlsx')\n",
        "sheet1 = wb.worksheets[0]\n",
        "rows, columns = sheet1.max_row, sheet1.max_column\n",
        "# print(rows)\n",
        "\n",
        "table_perf = client['thesis']['performance_table']\n",
        "temp = []\n",
        "now_sheet = read_xlsx('/content/drive/MyDrive/myThesis/test001/Testing/power_30Models.xlsx')\n",
        "\n",
        "start = time.time()\n",
        "for i in range(1, 61):\n",
        "  tmp_model_id = read_xlsx_cell(now_sheet, i+1, 11)\n",
        "#   print(i, tmp_model_id)\n",
        "  model_id = table_model.find_one({'model_loc': {'$regex': tmp_model_id}})['_id']\n",
        "  # \n",
        "\n",
        "  # obtaining individual ObjectID of test_data\n",
        "  tmp_test_data_index = int(float(read_xlsx_cell(now_sheet, i+1, 2)))\n",
        "  tmp_path = '/content/drive/My Drive/myThesis/test001/Training/scenario' + str(tmp_test_data_index)\n",
        "  test_data = table_data.find_one({'data_path': tmp_path})['_id']\n",
        "\n",
        "  temp_metric_dict = {'task_category': 'prediction',\n",
        "                      '500-MAE': float(read_xlsx_cell(now_sheet, i+1, 3)),\n",
        "                      '500-RMSE': float(read_xlsx_cell(now_sheet, i+1, 4)),\n",
        "                      '300-MAE': float(read_xlsx_cell(now_sheet, i+1, 5)),\n",
        "                      '300-RMSE': float(read_xlsx_cell(now_sheet, i+1, 6)),\n",
        "                      '150-MAE': float(read_xlsx_cell(now_sheet, i+1, 7)),\n",
        "                      '150-RMSE': float(read_xlsx_cell(now_sheet, i+1, 8)),\n",
        "                      'Inference_Time': float((read_xlsx_cell(now_sheet, i+1, 5))[:-1])\n",
        "                    #   'epoch_to_quality': int(float(read_xlsx(i+1, 7)))\n",
        "                      }\n",
        "  temp.append(entry_dict_perf(model_id, test_data, temp_metric_dict))\n",
        "\n",
        "table_perf.insert_many(temp)\n",
        "end = time.time()\n",
        "# print(temp)\n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22.24265146255493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Iwnsd4yqt94"
      },
      "source": [
        "table_perf = client['thesis']['performance_table']\n",
        "table_perf.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtgX-RPEg-_A"
      },
      "source": [
        "####尝试查询"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eetEvyG_gmM2"
      },
      "source": [
        "table_model1 = client['thesis']['test_model_table']\n",
        "table_data1 = client['thesis']['test_data_table']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt2r_ecijg-S",
        "outputId": "742d1d8a-83aa-4748-a405-a6bffea95185"
      },
      "source": [
        "myquery = {\"pattern.min\":  {'$gt': 59.97, '$lt': 59.992}}\n",
        "haha = table_data1.find(myquery)\n",
        "print(list(haha))\n",
        "# def closest(test_query, check_list, num_of_entries):\n",
        "#     tracer = check_list[0]\n",
        "#     tmp_lst = list(tracer['pattern'].values())\n",
        "#     test_q_list = list(test_query['pattern'].values())\n",
        "#     dist = ((tmp_lst[0]-test_q_list[0])**2 + \\\n",
        "#             (tmp_lst[1]-test_q_list[1])**2 + \\\n",
        "#             (tmp_lst[2]-test_q_list[2])**2 + \\\n",
        "#             (tmp_lst[3]-test_q_list[3])**2) ** 0.5\n",
        "\n",
        "#     tmp_dict = {tracer['_id']: dist}\n",
        "    \n",
        "#     for item in check_list:\n",
        "#         tmp_lst = list(item['pattern'].values())\n",
        "#         current_dist = ((tmp_lst[0]-test_q_list[0])**2 + \\\n",
        "#                         (tmp_lst[1]-test_q_list[1])**2 + \\\n",
        "#                         (tmp_lst[2]-test_q_list[2])**2 + \\\n",
        "#                         (tmp_lst[3]-test_q_list[3])**2) ** 0.5\n",
        "#         if current_dist >= dist:\n",
        "#             pass\n",
        "#         else:\n",
        "#             print(dist)\n",
        "#             dist = current_dist\n",
        "#             tracer = item\n",
        "#     closest_query_id = tracer['_id']\n",
        "#     return closest_query_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'_id': ObjectId('5fbedc24b6e9aea1ff39d82e'), 'pattern': {'min': 59.9895, 'max': 60.0224, 'avg': 60.0057, 'std': 0.0032, 'no_of_samples': 1375}, 'measurement': 'frequency', 'data_path': '/content/drive/My Drive/myThesis/test001/Training/scenario3'}, {'_id': ObjectId('5fbedc24b6e9aea1ff39d82f'), 'pattern': {'min': 59.9911, 'max': 60.0034, 'avg': 59.9998, 'std': 0.0011, 'no_of_samples': 1375}, 'measurement': 'frequency', 'data_path': '/content/drive/My Drive/myThesis/test001/Training/scenario4'}, {'_id': ObjectId('5fbedc24b6e9aea1ff39d830'), 'pattern': {'min': 59.9845, 'max': 60.0076, 'avg': 59.9968, 'std': 0.0016, 'no_of_samples': 1375}, 'measurement': 'frequency', 'data_path': '/content/drive/My Drive/myThesis/test001/Training/scenario5'}, {'_id': ObjectId('5fbedc24b6e9aea1ff39d832'), 'pattern': {'min': 59.9842, 'max': 60.0169, 'avg': 60.0032, 'std': 0.0025, 'no_of_samples': 1375}, 'measurement': 'frequency', 'data_path': '/content/drive/My Drive/myThesis/test001/Training/scenario7'}, {'_id': ObjectId('5fbedc24b6e9aea1ff39d833'), 'pattern': {'min': 59.9771, 'max': 59.9995, 'avg': 59.9939, 'std': 0.0035, 'no_of_samples': 1375}, 'measurement': 'frequency', 'data_path': '/content/drive/My Drive/myThesis/test001/Training/scenario8'}, {'_id': ObjectId('5fbedc24b6e9aea1ff39d834'), 'pattern': {'min': 59.9748, 'max': 60.015, 'avg': 59.9938, 'std': 0.0036, 'no_of_samples': 1375}, 'measurement': 'frequency', 'data_path': '/content/drive/My Drive/myThesis/test001/Training/scenario9'}, {'_id': ObjectId('5fbedc24b6e9aea1ff39d835'), 'pattern': {'min': 59.9853, 'max': 60.0129, 'avg': 59.9972, 'std': 0.0018, 'no_of_samples': 1375}, 'measurement': 'frequency', 'data_path': '/content/drive/My Drive/myThesis/test001/Training/scenario10'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOK4vArzlR2c"
      },
      "source": [
        "def closest(test_query, num_of_entries):\n",
        "\n",
        "    check_list = list(table_data.find({}, {\"pattern.min\":  1, \n",
        "                                           \"pattern.max\":  1, \n",
        "                                           \"pattern.avg\":  1, \n",
        "                                           \"pattern.std\":  1}))\n",
        "\n",
        "    test_q_list = list(test_query['pattern'].values())\n",
        "    tmp_dict = {}\n",
        "    for item in check_list:\n",
        "        tmp_lst = list(item['pattern'].values())\n",
        "        current_dist = ((tmp_lst[0]-test_q_list[0])**2 + \\\n",
        "                        (tmp_lst[1]-test_q_list[1])**2 + \\\n",
        "                        (tmp_lst[2]-test_q_list[2])**2 + \\\n",
        "                        (tmp_lst[3]-test_q_list[3])**2) ** 0.5\n",
        "        tmp_dict[item['_id']] = current_dist\n",
        "        \n",
        "    L = sorted(tmp_dict.items(), key=lambda item:item[1], reverse=False)\n",
        "    if num_of_entries > 0:\n",
        "        L = L[:num_of_entries]\n",
        "    else:\n",
        "        pass\n",
        "    dictdata = {}\n",
        "    for l in L:\n",
        "        dictdata[l[0]] = l[1]    \n",
        "    return dictdata\n",
        "\n",
        "\n",
        "def test_finder(test_query_data, num_of_entries, expected_matches, expected_metric):\n",
        "    try:\n",
        "        closest_query_dataID_dist_dict = closest(test_query_data, num_of_entries)\n",
        "        # Top-N与data_entry相似的data\n",
        "\n",
        "        query_id_trainData = list(table_model.find({}, {\"training_data\":  1}))\n",
        "        modelID_trainData_dict = {}\n",
        "        for item in query_id_trainData:\n",
        "            modelID_trainData_dict[list(item.values())[0]] = list(item.values())[1]\n",
        "        # inter = set(closest_query_dataID_dist_dict.keys()).intersection(set(modelID_trainData_dict.keys()))\n",
        "        # print(modelID_trainData_dict)\n",
        "        tmp_matched_models = set()\n",
        "        for trainDataDict_of_one_model in modelID_trainData_dict:\n",
        "            tmp_set1 = set(closest_query_dataID_dist_dict.keys())\n",
        "            tmp_set2 = set(modelID_trainData_dict[trainDataDict_of_one_model])\n",
        "            if tmp_set1.intersection(tmp_set2):\n",
        "                tmp_matched_models.add(trainDataDict_of_one_model) # matched modelIDs\n",
        "                # 所有包含与data_entry相似的data的models\n",
        "        \n",
        "        matched_perf = []\n",
        "        tmp_metric = 'task.' + expected_metric\n",
        "        perf_entry = table_perf.find({}, {'model_id': 1, tmp_metric: 1})\n",
        "        for item in tmp_matched_models:\n",
        "            # tmp_perf_entry = table_perf.find_one({'model_id': item})\n",
        "            tmp_perf_entry = table_perf.find({\"model_id\":  item})\n",
        "            tn = list(tmp_perf_entry)\n",
        "            # print(tn, len(tn))\n",
        "            if len(tn) != 0:\n",
        "                # print(tn)\n",
        "                matched_perf += tn\n",
        "        \n",
        "        new_s = sorted(matched_perf, key = lambda e:e.__getitem__('task').__getitem__(expected_metric))\n",
        "\n",
        "        return new_s[:expected_matches] # in set/dict format\n",
        "    except ValueError:\n",
        "        print('Invalid list, please check again')\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QfA0-nKSRk9",
        "outputId": "c1c20f34-3e39-47ce-8182-d9127368b5cb"
      },
      "source": [
        "full_data_table = list(table_data.find({}))\n",
        "search = full_data_table[6]\n",
        "# print(search)\n",
        "print(f\"Let's search: {search['pattern']}\")\n",
        "# print(closest(search, -1))\n",
        "print(test_finder(search, -1, 10, 'Inference_Time'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's search: {'min': 59.9771, 'max': 59.9995, 'avg': 59.9939, 'std': 0.0035, 'no_of_samples': 1375}\n",
            "[{'_id': ObjectId('5fc52bddf9ad7c82692b9aca'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a92'), 'test_data': ObjectId('5fbeb895e98c620967f2ede7'), 'task': {'task_category': 'prediction', '500-MAE': 0.00020964, '500-RMSE': 0.00032066, '300-MAE': 0.0002755, '300-RMSE': 0.00039364, '150-MAE': 0.00034659, '150-RMSE': 0.00047904, 'Inference_Time': 0.000275}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9acc'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a94'), 'test_data': ObjectId('5fbeb895e98c620967f2ee12'), 'task': {'task_category': 'prediction', '500-MAE': 0.00033437, '500-RMSE': 0.00043366, '300-MAE': 0.00036871, '300-RMSE': 0.0004788, '150-MAE': 0.00036713, '150-RMSE': 0.0005065, 'Inference_Time': 0.0003687}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9acf'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a9e'), 'test_data': ObjectId('5fbeb895e98c620967f2ee1f'), 'task': {'task_category': 'prediction', '500-MAE': 0.00049162, '500-RMSE': 0.00061457, '300-MAE': 0.00052413, '300-RMSE': 0.00070247, '150-MAE': 0.00066964, '150-RMSE': 0.00090922, 'Inference_Time': 0.0005241}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9ade'), 'model_id': ObjectId('5fc526cff9ad7c82692b9aa5'), 'test_data': ObjectId('5fbeb895e98c620967f2ee75'), 'task': {'task_category': 'prediction', '500-MAE': 0.00062289, '500-RMSE': 0.00078068, '300-MAE': 0.00062758, '300-RMSE': 0.00079129, '150-MAE': 0.0005893, '150-RMSE': 0.00077382, 'Inference_Time': 0.0006275}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9ac9'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a92'), 'test_data': ObjectId('5fbeb895e98c620967f2ede6'), 'task': {'task_category': 'prediction', '500-MAE': 0.00063715, '500-RMSE': 0.00075801, '300-MAE': 0.00068129, '300-RMSE': 0.00083547, '150-MAE': 0.0007603, '150-RMSE': 0.00094008, 'Inference_Time': 0.0006812}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9ad1'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a96'), 'test_data': ObjectId('5fbeb895e98c620967f2ee2c'), 'task': {'task_category': 'prediction', '500-MAE': 0.00069229, '500-RMSE': 0.00098019, '300-MAE': 0.00076862, '300-RMSE': 0.00109308, '150-MAE': 0.00109516, '150-RMSE': 0.00139741, 'Inference_Time': 0.0007686}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9adc'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a94'), 'test_data': ObjectId('5fbeb895e98c620967f2ee66'), 'task': {'task_category': 'prediction', '500-MAE': 0.0009008, '500-RMSE': 0.00096779, '300-MAE': 0.00087281, '300-RMSE': 0.00097567, '150-MAE': 0.00072841, '150-RMSE': 0.00087796, 'Inference_Time': 0.0008728}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9ae1'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a98'), 'test_data': ObjectId('5fbeb895e98c620967f2ee91'), 'task': {'task_category': 'prediction', '500-MAE': 0.0008296, '500-RMSE': 0.00127493, '300-MAE': 0.00089525, '300-RMSE': 0.00123516, '150-MAE': 0.00100161, '150-RMSE': 0.00133943, 'Inference_Time': 0.0008952}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9abc'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a94'), 'test_data': ObjectId('5fbeb895e98c620967f2ed84'), 'task': {'task_category': 'prediction', '500-MAE': 0.00071009, '500-RMSE': 0.00121285, '300-MAE': 0.00094883, '300-RMSE': 0.00151886, '150-MAE': 0.00156819, '150-RMSE': 0.00210406, 'Inference_Time': 0.0009488}}, {'_id': ObjectId('5fc52bddf9ad7c82692b9adb'), 'model_id': ObjectId('5fc526cff9ad7c82692b9a94'), 'test_data': ObjectId('5fbeb895e98c620967f2ee65'), 'task': {'task_category': 'prediction', '500-MAE': 0.00080059, '500-RMSE': 0.00111629, '300-MAE': 0.00106391, '300-RMSE': 0.00138524, '150-MAE': 0.0015919, '150-RMSE': 0.00184537, 'Inference_Time': 0.0010639}}]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}